{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import importlib\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import wasserstein_distance\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from geomloss import SamplesLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('usps.h5', 'r') as hf:\n",
    "        train_usps = hf.get('train')\n",
    "        X_train_usps = train_usps.get('data')[:]\n",
    "        y_train_usps = train_usps.get('target')[:]\n",
    "        test_usps = hf.get('test')\n",
    "        X_test_usps = test_usps.get('data')[:]\n",
    "        y_test_usps = test_usps.get('target')[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mnist(X_train_path, y_train_path, X_test_path, y_test_path):\n",
    "    with open(X_train_path, 'rb') as f:\n",
    "        # Skip the magic number and dimension info\n",
    "        f.read(16)\n",
    "        X_train_mnist = np.fromfile(f, dtype=np.uint8).reshape(-1, 1, 28, 28)\n",
    "\n",
    "    with open(y_train_path, 'rb') as f:\n",
    "        # Skip the magic number and dimension info\n",
    "        f.read(8)\n",
    "        y_train_mnist = np.fromfile(f, dtype=np.uint8)\n",
    "        \n",
    "    with open(X_test_path, 'rb') as f:\n",
    "        # Skip the magic number and dimension info\n",
    "        f.read(16)\n",
    "        X_test_mnist = np.fromfile(f, dtype=np.uint8).reshape(-1, 1, 28, 28)\n",
    "        \n",
    "    with open(y_test_path, 'rb') as f:\n",
    "        # Skip the magic number and dimension info\n",
    "        f.read(8)\n",
    "        y_test_mnist = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "    return X_train_mnist, y_train_mnist, X_test_mnist, y_test_mnist\n",
    "\n",
    "# Load MNIST data\n",
    "X_train_path = \"mnist/mnist/archive/train-images-idx3-ubyte/train-images-idx3-ubyte\"\n",
    "y_train_path = \"mnist/mnist/archive/train-labels-idx1-ubyte/train-labels-idx1-ubyte\"\n",
    "X_test_path = \"mnist/mnist/archive/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\"\n",
    "y_test_path = \"mnist/mnist/archive/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\"\n",
    "X_train_mnist, y_train_mnist, X_test_mnist, y_test_mnist = read_mnist(X_train_path, y_train_path, X_test_path, y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datasets to PyTorch tensors\n",
    "X_train_mnist = torch.tensor(X_train_mnist).float()\n",
    "y_train_mnist = torch.tensor(y_train_mnist).long()\n",
    "X_test_mnist = torch.tensor(X_test_mnist).float()\n",
    "y_test_mnist = torch.tensor(y_test_mnist).long()\n",
    "\n",
    "# Ensure the data is reshaped correctly (no extra singleton dimension)\n",
    "X_train_mnist = X_train_mnist.reshape(-1, 1, 28, 28)  # MNIST images are 28x28\n",
    "X_test_mnist = X_test_mnist.reshape(-1, 1, 28, 28)  # MNIST images are 28x28\n",
    "\n",
    "\n",
    "# Normalize the datasets\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "X_train_mnist = mnist_transform(X_train_mnist)\n",
    "X_test_mnist = mnist_transform(X_test_mnist)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_dataset_mnist = TensorDataset(X_train_mnist, y_train_mnist)\n",
    "train_loader_mnist = DataLoader(train_dataset_mnist, shuffle=True)\n",
    "\n",
    "test_dataset_mnist = TensorDataset(X_test_mnist, y_test_mnist)\n",
    "test_loader_mnist = DataLoader(test_dataset_mnist, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class USPSDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert array to PIL Image\n",
    "        image = Image.fromarray(image.squeeze(), mode='L')\n",
    "\n",
    "        # Apply the transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "usps_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Resize images to 28x28\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "# Recreate the USPS dataset and dataloader\n",
    "train_dataset_usps = USPSDataset(X_train_usps, y_train_usps, transform=usps_transform)\n",
    "train_loader_usps = DataLoader(train_dataset_usps, shuffle=True)\n",
    "\n",
    "test_dataset_usps = USPSDataset(X_test_usps, y_train_usps, transform=usps_transform)\n",
    "test_loader_usps = DataLoader(test_dataset_usps, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(3 * 3 * 128, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10) \n",
    "        self.dropout = nn.Dropout(p=0.2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x) \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x) \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 3 * 3 * 128) \n",
    "        x = self.dropout(x)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute_wasserstein_distance(source_features, target_features):\n",
    "    source_features_flat = source_features.reshape(source_features.shape[0], -1)\n",
    "    target_features_flat = target_features.reshape(target_features.shape[0], -1)\n",
    "    \n",
    "    wd = 0\n",
    "    for i in range(source_features_flat.shape[1]):\n",
    "        wd += wasserstein_distance(source_features_flat[:, i], target_features_flat[:, i])\n",
    "    return wd / source_features_flat.shape[1]\n",
    "\n",
    "\n",
    "def train(model, source_loader, target_loader, num_epochs, criterion, weight_wasserstein, loss_function_str=None):\n",
    "    model.train()\n",
    "    feature_optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = StepLR(feature_optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    if loss_function_str == 'wasserstein':\n",
    "        wasserstein_loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for (source_data, source_labels), (target_data, _) in zip(source_loader, target_loader):\n",
    "            feature_optimizer.zero_grad()\n",
    "\n",
    "            source_features = model(source_data)\n",
    "            target_features = model(target_data)\n",
    "\n",
    "            classification_loss = criterion(source_features, source_labels)\n",
    "            wd_loss = 0\n",
    "\n",
    "            if loss_function_str == 'wasserstein':\n",
    "                wd_loss = wasserstein_loss(source_features, target_features)\n",
    "\n",
    "            total_loss = classification_loss + weight_wasserstein * wd_loss\n",
    "            total_loss.backward()\n",
    "            feature_optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, LR: {scheduler.get_last_lr()} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, LR: [2e-05] completed.\n",
      "Epoch 2/50, LR: [2e-05] completed.\n",
      "Epoch 3/50, LR: [2e-05] completed.\n",
      "Epoch 4/50, LR: [2e-05] completed.\n",
      "Epoch 5/50, LR: [2e-05] completed.\n",
      "Epoch 6/50, LR: [2e-05] completed.\n",
      "Epoch 7/50, LR: [2e-05] completed.\n",
      "Epoch 8/50, LR: [2e-05] completed.\n",
      "Epoch 9/50, LR: [2e-05] completed.\n",
      "Epoch 10/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 11/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 12/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 13/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 14/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 15/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 16/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 17/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 18/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 19/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 20/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 21/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 22/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 23/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 24/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 25/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 26/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 27/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 28/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 29/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 30/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 31/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 32/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 33/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 34/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 35/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 36/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 37/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 38/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 39/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 40/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 41/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 42/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 43/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 44/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 45/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 46/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 47/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 48/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 49/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 50/50, LR: [2.000000000000001e-10] completed.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractorCNN()\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 50\n",
    "loss_function_str = 'wasserstein'  # This is just a string to control the use of Wasserstein loss\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function object\n",
    "weight_wasserstein = 0.25\n",
    "\n",
    "# Start the training process\n",
    "train(feature_extractor, train_loader_mnist, train_loader_usps, num_epochs,\n",
    "      criterion, weight_wasserstein, loss_function_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 98.58%\n",
      "Accuracy of the model on the test images: 7.872446437468859%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            if data.shape[1] != 1:\n",
    "                raise ValueError(f\"Source data should have 1 channel, got {data.shape[1]}\")\n",
    "\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test images: {accuracy}%')\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Evaluate on MNIST Test Set\n",
    "mnist_test_labels, mnist_test_preds = evaluate_model(feature_extractor, test_loader_mnist)\n",
    "\n",
    "# Evaluate on USPS Test Set\n",
    "usps_test_labels, usps_test_preds = evaluate_model(feature_extractor, test_loader_usps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
