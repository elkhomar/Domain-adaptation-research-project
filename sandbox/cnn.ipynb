{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marionchadal/Library/Mobile Documents/.Trash/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/marionchadal/Library/Mobile Documents/.Trash/myenv/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import importlib\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import wasserstein_distance\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('usps.h5', 'r') as hf:\n",
    "        train_usps = hf.get('train')\n",
    "        X_train_usps = train_usps.get('data')[:]\n",
    "        y_train_usps = train_usps.get('target')[:]\n",
    "        test_usps = hf.get('test')\n",
    "        X_test_usps = test_usps.get('data')[:]\n",
    "        y_test_usps = test_usps.get('target')[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mnist(X_train_path, y_train_path, X_test_path, y_test_path):\n",
    "    with open(X_train_path, 'rb') as f:\n",
    "        # Skip the magic number and dimension info\n",
    "        f.read(16)\n",
    "        X_train_mnist = np.fromfile(f, dtype=np.uint8).reshape(-1, 1, 28, 28)\n",
    "\n",
    "    with open(y_train_path, 'rb') as f:\n",
    "        # Skip the magic number and dimension info\n",
    "        f.read(8)\n",
    "        y_train_mnist = np.fromfile(f, dtype=np.uint8)\n",
    "        \n",
    "    with open(X_test_path, 'rb') as f:\n",
    "        # Skip the magic number and dimension info\n",
    "        f.read(16)\n",
    "        X_test_mnist = np.fromfile(f, dtype=np.uint8).reshape(-1, 1, 28, 28)\n",
    "        \n",
    "    with open(y_test_path, 'rb') as f:\n",
    "        # Skip the magic number and dimension info\n",
    "        f.read(8)\n",
    "        y_test_mnist = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "    return X_train_mnist, y_train_mnist, X_test_mnist, y_test_mnist\n",
    "\n",
    "# Load MNIST data\n",
    "X_train_path = \"mnist/mnist/archive/train-images-idx3-ubyte/train-images-idx3-ubyte\"\n",
    "y_train_path = \"mnist/mnist/archive/train-labels-idx1-ubyte/train-labels-idx1-ubyte\"\n",
    "X_test_path = \"mnist/mnist/archive/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\"\n",
    "y_test_path = \"mnist/mnist/archive/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\"\n",
    "X_train_mnist, y_train_mnist, X_test_mnist, y_test_mnist = read_mnist(X_train_path, y_train_path, X_test_path, y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datasets to PyTorch tensors\n",
    "X_train_mnist = torch.tensor(X_train_mnist).float()\n",
    "y_train_mnist = torch.tensor(y_train_mnist).long()\n",
    "X_test_mnist = torch.tensor(X_test_mnist).float()\n",
    "y_test_mnist = torch.tensor(y_test_mnist).long()\n",
    "\n",
    "# Ensure the data is reshaped correctly (no extra singleton dimension)\n",
    "X_train_mnist = X_train_mnist.reshape(-1, 1, 28, 28)  # MNIST images are 28x28\n",
    "X_test_mnist = X_test_mnist.reshape(-1, 1, 28, 28)  # MNIST images are 28x28\n",
    "\n",
    "\n",
    "# Normalize the datasets\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "X_train_mnist = mnist_transform(X_train_mnist)\n",
    "X_test_mnist = mnist_transform(X_test_mnist)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_dataset_mnist = TensorDataset(X_train_mnist, y_train_mnist)\n",
    "train_loader_mnist = DataLoader(train_dataset_mnist, shuffle=True)\n",
    "\n",
    "test_dataset_mnist = TensorDataset(X_test_mnist, y_test_mnist)\n",
    "test_loader_mnist = DataLoader(test_dataset_mnist, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class USPSDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert array to PIL Image\n",
    "        image = Image.fromarray(image.squeeze(), mode='L')\n",
    "\n",
    "        # Apply the transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "usps_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Resize images to 28x28\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "# Recreate the USPS dataset and dataloader\n",
    "train_dataset_usps = USPSDataset(X_train_usps, y_train_usps, transform=usps_transform)\n",
    "train_loader_usps = DataLoader(train_dataset_usps, shuffle=True)\n",
    "\n",
    "test_dataset_usps = USPSDataset(X_test_usps, y_train_usps, transform=usps_transform)\n",
    "test_loader_usps = DataLoader(test_dataset_usps, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(3 * 3 * 128, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10) \n",
    "        self.dropout = nn.Dropout(p=0.2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x) \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x) \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 3 * 3 * 128) \n",
    "        x = self.dropout(x)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute_wasserstein_distance(source_features, target_features):\n",
    "    source_features_flat = source_features.reshape(source_features.shape[0], -1)\n",
    "    target_features_flat = target_features.reshape(target_features.shape[0], -1)\n",
    "    \n",
    "    wd = 0\n",
    "    for i in range(source_features_flat.shape[1]):\n",
    "        wd += wasserstein_distance(source_features_flat[:, i], target_features_flat[:, i])\n",
    "    return wd / source_features_flat.shape[1]\n",
    "\n",
    "\n",
    "def train(model, source_loader, target_loader, num_epochs, criterion, weight_wasserstein, loss_function_str=None):\n",
    "    model.train()\n",
    "    feature_optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = StepLR(feature_optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    if loss_function_str == 'wasserstein':\n",
    "        wasserstein_loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for (source_data, source_labels), (target_data, _) in zip(source_loader, target_loader):\n",
    "            feature_optimizer.zero_grad()\n",
    "\n",
    "            source_features = model(source_data)\n",
    "            target_features = model(target_data)\n",
    "\n",
    "            classification_loss = criterion(source_features, source_labels)\n",
    "            wd_loss = 0\n",
    "\n",
    "            if loss_function_str == 'wasserstein':\n",
    "                wd_loss = wasserstein_loss(source_features, target_features)\n",
    "\n",
    "            total_loss = classification_loss + weight_wasserstein * wd_loss\n",
    "            total_loss.backward()\n",
    "            feature_optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, LR: {scheduler.get_last_lr()} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, LR: [2e-05] completed.\n",
      "Epoch 2/50, LR: [2e-05] completed.\n",
      "Epoch 3/50, LR: [2e-05] completed.\n",
      "Epoch 4/50, LR: [2e-05] completed.\n",
      "Epoch 5/50, LR: [2e-05] completed.\n",
      "Epoch 6/50, LR: [2e-05] completed.\n",
      "Epoch 7/50, LR: [2e-05] completed.\n",
      "Epoch 8/50, LR: [2e-05] completed.\n",
      "Epoch 9/50, LR: [2e-05] completed.\n",
      "Epoch 10/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 11/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 12/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 13/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 14/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 15/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 16/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 17/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 18/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 19/50, LR: [2.0000000000000003e-06] completed.\n",
      "Epoch 20/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 21/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 22/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 23/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 24/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 25/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 26/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 27/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 28/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 29/50, LR: [2.0000000000000004e-07] completed.\n",
      "Epoch 30/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 31/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 32/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 33/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 34/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 35/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 36/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 37/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 38/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 39/50, LR: [2.0000000000000007e-08] completed.\n",
      "Epoch 40/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 41/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 42/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 43/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 44/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 45/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 46/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 47/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 48/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 49/50, LR: [2.000000000000001e-09] completed.\n",
      "Epoch 50/50, LR: [2.000000000000001e-10] completed.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractorCNN()\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 50\n",
    "loss_function_str = 'wasserstein'  # This is just a string to control the use of Wasserstein loss\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function object\n",
    "weight_wasserstein = 0.25\n",
    "\n",
    "# Start the training process\n",
    "train(feature_extractor, train_loader_mnist, train_loader_usps, num_epochs,\n",
    "      criterion, weight_wasserstein, loss_function_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 98.58%\n",
      "Accuracy of the model on the test images: 7.872446437468859%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            if data.shape[1] != 1:\n",
    "                raise ValueError(f\"Source data should have 1 channel, got {data.shape[1]}\")\n",
    "\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test images: {accuracy}%')\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Evaluate on MNIST Test Set\n",
    "mnist_test_labels, mnist_test_preds = evaluate_model(feature_extractor, test_loader_mnist)\n",
    "\n",
    "# Evaluate on USPS Test Set\n",
    "usps_test_labels, usps_test_preds = evaluate_model(feature_extractor, test_loader_usps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ot import solve_sample\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 12708641.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 24265964.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 9709907.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 7361100.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_usps, test_usps = utils.load_usps('../Digit-Five/usps_28x28.pkl', batch_size=64)\n",
    "train_mnist, test_mnist = utils.load_mnist(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, source_loader, target_loader, num_epochs, criterion, weight_wasserstein, loss_function_str=None):\n",
    "    model.train()\n",
    "    feature_optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = StepLR(feature_optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for (source_data, source_labels), (target_data, _) in zip(source_loader, target_loader):\n",
    "            feature_optimizer.zero_grad()\n",
    "\n",
    "            source_features = model(source_data).detach()\n",
    "            target_features = model(target_data).detach()\n",
    "\n",
    "            source_features_flat = source_features.view(source_features.size(0), -1).cpu().numpy()\n",
    "            target_features_flat = target_features.view(target_features.size(0), -1).cpu().numpy()\n",
    "\n",
    "            classification_loss = criterion(model(source_data), source_labels)\n",
    "            wd_loss = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "            if loss_function_str == 'wasserstein':\n",
    "                # Call solve_sample and extract the Wasserstein distance or desired metric\n",
    "                result = solve_sample(source_features_flat, target_features_flat, metric='sqeuclidean', reg=1e-3, method='sinkhorn')\n",
    "                if hasattr(result, 'value'):  # Assuming 'value' holds the Wasserstein distance\n",
    "                    wd_loss_value = result.value  # Extract the numerical value\n",
    "                    wd_loss = torch.tensor(wd_loss_value, dtype=torch.float32, requires_grad=True).to(source_features.device)\n",
    "\n",
    "            total_loss = classification_loss + weight_wasserstein * wd_loss\n",
    "            total_loss.backward()\n",
    "            feature_optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, LR: {scheduler.get_last_lr()[0]} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/marionchadal/Library/Mobile Documents/com~apple~CloudDocs/M2DS 2/ML Research Seminar/Domain-adaptation-research-project/sandbox/cnn.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marionchadal/Library/Mobile%20Documents/com~apple~CloudDocs/M2DS%202/ML%20Research%20Seminar/Domain-adaptation-research-project/sandbox/cnn.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m weight_wasserstein \u001b[39m=\u001b[39m \u001b[39m0.25\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marionchadal/Library/Mobile%20Documents/com~apple~CloudDocs/M2DS%202/ML%20Research%20Seminar/Domain-adaptation-research-project/sandbox/cnn.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Start the training process\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marionchadal/Library/Mobile%20Documents/com~apple~CloudDocs/M2DS%202/ML%20Research%20Seminar/Domain-adaptation-research-project/sandbox/cnn.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train(feature_extractor, train_mnist, train_usps, num_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marionchadal/Library/Mobile%20Documents/com~apple~CloudDocs/M2DS%202/ML%20Research%20Seminar/Domain-adaptation-research-project/sandbox/cnn.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m       criterion, weight_wasserstein, loss_function_str)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marionchadal/Library/Mobile%20Documents/com~apple~CloudDocs/M2DS%202/ML%20Research%20Seminar/Domain-adaptation-research-project/sandbox/cnn.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m a \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/Users/marionchadal/Library/Mobile Documents/com~apple~CloudDocs/M2DS 2/ML Research Seminar/Domain-adaptation-research-project/sandbox/cnn.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marionchadal/Library/Mobile%20Documents/com~apple~CloudDocs/M2DS%202/ML%20Research%20Seminar/Domain-adaptation-research-project/sandbox/cnn.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m source_features_flat \u001b[39m=\u001b[39m source_features\u001b[39m.\u001b[39mview(source_features\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marionchadal/Library/Mobile%20Documents/com~apple~CloudDocs/M2DS%202/ML%20Research%20Seminar/Domain-adaptation-research-project/sandbox/cnn.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m target_features_flat \u001b[39m=\u001b[39m target_features\u001b[39m.\u001b[39mview(target_features\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marionchadal/Library/Mobile%20Documents/com~apple~CloudDocs/M2DS%202/ML%20Research%20Seminar/Domain-adaptation-research-project/sandbox/cnn.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m classification_loss \u001b[39m=\u001b[39m criterion(model(source_data), source_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marionchadal/Library/Mobile%20Documents/com~apple~CloudDocs/M2DS%202/ML%20Research%20Seminar/Domain-adaptation-research-project/sandbox/cnn.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m wd_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marionchadal/Library/Mobile%20Documents/com~apple~CloudDocs/M2DS%202/ML%20Research%20Seminar/Domain-adaptation-research-project/sandbox/cnn.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m loss_function_str \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mwasserstein\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marionchadal/Library/Mobile%20Documents/com~apple~CloudDocs/M2DS%202/ML%20Research%20Seminar/Domain-adaptation-research-project/sandbox/cnn.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# Call solve_sample and extract the Wasserstein distance or desired metric\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/.Trash/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/.Trash/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/.Trash/myenv/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/.Trash/myenv/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractorCNN()\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "loss_function_str = 'wasserstein'  # This is just a string to control the use of Wasserstein loss\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function object\n",
    "weight_wasserstein = 0.25\n",
    "\n",
    "# Start the training process\n",
    "train(feature_extractor, train_mnist, train_usps, num_epochs,\n",
    "      criterion, weight_wasserstein, loss_function_str)\n",
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on MNIST Test Set\n",
    "mnist_test_labels, mnist_test_preds = evaluate_model(feature_extractor, test_mnist)\n",
    "\n",
    "# Evaluate on USPS Test Set\n",
    "usps_test_labels, usps_test_preds = evaluate_model(feature_extractor, test_usps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
